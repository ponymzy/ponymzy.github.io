
# üìù Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='pub_images/neural-rdm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Residual Diffusion Models for Deep Scalable Vision Generation](https://arxiv.org/pdf/2406.13215)

**Zhiyuan Ma**, Liangliang Zhao, Biqing Qi, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- a simple yet meaningful change to the common architecture of deep generative networks by introducing a series of learnable gated residual parameters that conform to the generative dynamics that facilitates effective denoising, dynamical isometry and enables the stable training of extremely deep networks.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='pub_images/safe-sd-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger for Invisible Generative Watermarking](https://arxiv.org/pdf/2407.13188)

**Zhiyuan Ma**, Guoli Jia, Biqing Qi, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A safe and high-traceable Stable Diffusion framework (namely Safe-SD) to adaptively implant the graphical watermarks (e.g., QR code) into the imperceptible structure-related pixels.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='pub_images/lmd-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LMD: faster image reconstruction with latent masking diffusion](https://ojs.aaai.org/index.php/AAAI/article/view/28209)

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A simple but faster image reconstruction framework with Latent Masking Diffusion, which stands on the shoulder of *DPMs* and *MAEs*.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='pub_images/adapedit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing](https://ojs.aaai.org/index.php/AAAI/article/view/28210)

**Zhiyuan Ma**, Guoli Jia, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A spatio-temporal guided adaptive editing algorithm, which realizes adaptive image editing by introducing a soft-attention strategy to dynamically vary the guiding degree from the editing conditions to visual pixels from both temporal and spatial perspectives.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='pub_images/gemkr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generative multi-modal knowledge retrieval with large language models](https://ojs.aaai.org/index.php/AAAI/article/view/29837)

Xinwei Long, Jiali Zeng, Fandong Meng, **Zhiyuan Ma**, et al.

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- An end-to-end generative framework for multi-modal knowledge retrieval by taking advantage of the fact within LLMs can effectively serve as virtual knowledge bases, even when trained with limited data.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='pub_images/hybrid-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HybridPrompt: bridging language models and human priors in prompt tuning for visual question answering](https://ojs.aaai.org/index.php/AAAI/article/view/26569)

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A *cloze-* and *verify-* style hybrid prompt framework with bridging language models and human priors in prompt tuning for VQA.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2022</div><img src='pub_images/cmal-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Cmal: A novel cross-modal associative learning framework for vision-language pre-training](https://dl.acm.org/doi/abs/10.1145/3503161.3548292)

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A novel cross-modal associative learning model with anchor points detection and cross-modal associative learning for vision-language pre-training. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2022</div><img src='pub_images/glaf-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GLAF: global-to-local aggregation and fission network for semantic level fact verification](https://aclanthology.org/2022.coling-1.155.pdf)

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- we introduce a fresh perspective to revisit the fact verification task and propose a novel Global-to-Local Aggregation and Fission Network (GLAF) to capture latent logical relations hidden in evidence clues for more accurate fact verification.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2022</div><img src='pub_images/unitranser-3.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[UniTranSeR: A unified transformer semantic representation framework for multimodal task-oriented dialog system](https://aclanthology.org/2022.acl-long.9.pdf)

**Zhiyuan Ma**, Jianjun Li, Guohui Li, Yongjing Cheng

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A unified (vision, language, knowledge..) Transformer semantic representation framework with feature alignment and intention reasoning, referred to UniTranSeR, for multimodal task-oriented dialog systems.
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2021</div><img src='pub_images/irnet-4.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Intention reasoning network for multi-domain end-to-end task-oriented dialogue](https://aclanthology.org/2021.emnlp-main.174.pdf)

**Zhiyuan Ma**, Jianjun Li, Zezheng Zhang, Guohui Li, Yongjing Cheng

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- A novel **intention mechanism** to better model deterministic entity knowledge for joint and multi-hop reasoning in multi-domain end-to-end task-oriented dialogue.
</div>
</div>


# üìù Selected Papers
- ``NeurIPS 2024`` [Neural Residual Diffusion Models for Deep Scalable Vision Generation](https://arxiv.org/pdf/2406.13215). **Zhiyuan Ma**, Liangliang Zhao, Biqing Qi, Bowen Zhou.
- ``NeurIPS 2024`` <span style="color:red">(Spotlight)</span> [Ultramedical: Building specialized generalists in biomedicine](https://arxiv.org/pdf/2406.03949). Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, **Zhiyuan Ma**, et al.
- ``NeurIPS 2024`` [Exploring Adversarial Robustness of Deep State Space Models](https://arxiv.org/pdf/2406.05532). Biqing Qi, Yang Luo, Junqi Gao, Pengfei Li, Kai Tian, **Zhiyuan Ma**, et al.
- ``ACM MM 2024`` [Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger for Invisible Generative Watermarking](https://arxiv.org/pdf/2407.13188). **Zhiyuan Ma**, Guoli Jia, et al.
- ``AAAI 2024``  [LMD: faster image reconstruction with latent masking diffusion](https://ojs.aaai.org/index.php/AAAI/article/view/28209). **Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Bowen Zhou.
- ``AAAI 2024`` [AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing](https://ojs.aaai.org/index.php/AAAI/article/view/28210). **Zhiyuan Ma**, Guoli Jia, Bowen Zhou.
- ``AAAI 2024``  [Generative multi-modal knowledge retrieval with large language models](https://ojs.aaai.org/index.php/AAAI/article/view/29837). Xinwei Long, Jiali Zeng, Fandong Meng, **Zhiyuan Ma**, et al.
- ``AAAI 2023`` <span style="color:red">(Oral)</span> [HybridPrompt: bridging language models and human priors in prompt tuning for visual question answering](https://ojs.aaai.org/index.php/AAAI/article/view/26569). **Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li.
- ``ACM MM 2022`` <span style="color:red">(Oral)</span> [Cmal: A novel cross-modal associative learning framework for vision-language pre-training](https://dl.acm.org/doi/abs/10.1145/3503161.3548292). **Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li.
- ``COLING 2022`` [GLAF: global-to-local aggregation and fission network for semantic level fact verification](https://aclanthology.org/2022.coling-1.155.pdf). **Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li.
- ``ACL 2022`` [UniTranSeR: A unified transformer semantic representation framework for multimodal task-oriented dialog system](https://aclanthology.org/2022.acl-long.9.pdf). **Zhiyuan Ma**, Jianjun Li, Guohui Li, Yongjing Cheng.
- ``EMNLP 2021`` [Intention reasoning network for multi-domain end-to-end task-oriented dialogue](https://aclanthology.org/2021.emnlp-main.174.pdf). **Zhiyuan Ma**, Jianjun Li, Zezheng Zhang, Guohui Li, Yongjing Cheng.
